from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta
import os
import requests
from dotenv import load_dotenv

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
SUPABASE_TABLE = "job_posts"

def scrape_jumpit_jobs():
  jobs = []
  with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    page = browser.new_page()
    page.goto("https://jumpit.saramin.co.kr/positions?jobCategory=2&sort=reg_dt")
    
    print("ìŠ¤í¬ë¡¤ ì „ì²´ ê³µê³  ë¡œë“œ ì¤‘..")
    prev_height = 0
    for _ in range(10):
      page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
      page.wait_for_timeout(1500)
      curr_height = page.evaluate("document.body.scrollHeight")
      if curr_height == prev_height:
        break
      prev_height = curr_height

    try:
      print("ì…€ë ‰í„° ëŒ€ê¸° ì¤‘.")
      page.wait_for_selector("div.sc-d609d44f-0", timeout=10000) 
    except Exception as e:
      print("ì…€ë ‰í„° ëŒ€ê¸° ì‹¤íŒ¨", e)
      browser.close()
      return []

    job_cards = page.locator("div.sc-d609d44f-0")
    count = job_cards.count()
    print(f"ì´ {count}ê°œì˜ ê³µê³  ê°ì§€")

    for i in range(count):
        card = job_cards.nth(i)
        try:
          title = "Unknown"
          if card.locator("h2.position_card_info_title").is_visible():
            title = card.locator("h2.position_card_info_title").inner_text()

          company = "Unknown"
          company_el = card.locator("div.dyMWAC span")
          if company_el.count() > 0:
            company = company_el.first.inner_text(timeout=2000)

          # ê¸°ìˆ  ìŠ¤íƒ ë¦¬ìŠ¤íŠ¸
          skill_spans = card.locator("ul.iFMgIl li span")
          skill_list = [skill_spans.nth(j).inner_text(timeout=2000) for j in range(skill_spans.count())]

          # ìœ„ì¹˜ & ê²½ë ¥ ì •ë³´
          detail_spans = card.locator("ul.cdeuol li")
          details = [detail_spans.nth(j).inner_text(timeout=2000) for j in range(detail_spans.count())]

          link = card.locator("a").first.get_attribute("href")
          if link and not link.startswith("http"):
              link = f"https://jumpit.saramin.co.kr{link}"

          jobs.append({
            "title": title.strip(),
            "company": company.strip(),
            "skills": skill_list,
            "details": details,
            "link": link,
            "source": "jumpit",
            "posted_date": datetime.today().date().isoformat(),
          })
        except Exception as e:
          print(f"âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜: [{i+1}/{count}]:", e)
          continue

    browser.close()
  return jobs

def upload_to_supabase_and_filter_new(jobs):
  today = datetime.today().date().isoformat()
  yesterday = (datetime.today() - timedelta(days=1)).date().isoformat()

  headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
  }

  query = f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}?select=link,posted_date&or=(posted_date.eq.{yesterday},posted_date.eq.{today})"
  existing = requests.get(query, headers=headers)

  if existing.status_code != 200:
    print("ì¡°íšŒ ì‹¤íŒ¨ supabase", existing.text)
    return []

  existing_links = {item["link"] for item in existing.json()}
  new_jobs = [job for job in jobs if job["link"] not in existing_links]

  if new_jobs:
      res = requests.post(
        f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}",
        headers=headers,
        json=new_jobs
      )
      if res.status_code not in [200, 201]:
        print("âŒ Supabase ì €ì¥ ì‹¤íŒ¨:", res.text)
      else:
        print(f"âœ… Supabaseì— {len(new_jobs)}ê°œ ì €ì¥ ì™„ë£Œ")
  else:
    print("ğŸ“­ ìƒˆë¡œìš´ ê³µê³  ì—†ìŒ")

  return new_jobs

if __name__ == "__main__":
  job_list = scrape_jumpit_jobs()
  print(f"ì´ {len(job_list)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ")

  for job in job_list:
    print("ğŸ“", job["title"], "|", job["company"], "|", job.get("link"))

  new_jobs = upload_to_supabase_and_filter_new(job_list)
  print(f"ğŸ†• ì˜¤ëŠ˜ ìƒˆë¡œ ì¶”ê°€ëœ ê³µê³  {len(new_jobs)}ê°œ:")
  for job in new_jobs:
    print("-", job["title"], "|", job["company"])
from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta
import os
import requests
from dotenv import load_dotenv

load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_ANON_KEY")
SUPABASE_TABLE = "job_posts"

def scrape_jumpit_jobs():
  jobs = []
  with sync_playwright() as p:
    browser = p.chromium.launch(headless=True)
    page = browser.new_page()
    page.goto("https://jumpit.saramin.co.kr/positions?jobCategory=2&sort=reg_dt")
    
    print("ìŠ¤í¬ë¡¤ ì „ì²´ ê³µê³  ë¡œë“œ ì¤‘..")
    prev_height = 0
    for _ in range(10):
      page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
      page.wait_for_timeout(1500)
      curr_height = page.evaluate("document.body.scrollHeight")
      if curr_height == prev_height:
        break
      prev_height = curr_height

    try:
      print("ì…€ë ‰í„° ëŒ€ê¸° ì¤‘.")
      page.wait_for_selector("div.sc-d609d44f-0", timeout=10000) 
    except Exception as e:
      print("ì…€ë ‰í„° ëŒ€ê¸° ì‹¤íŒ¨", e)
      browser.close()
      return []

    job_cards = page.locator("div.sc-d609d44f-0")
    count = job_cards.count()
    print(f"ì´ {count}ê°œì˜ ê³µê³  ê°ì§€")

    for i in range(count):
        card = job_cards.nth(i)
        try:
          title = "Unknown"
          if card.locator("h2.position_card_info_title").is_visible():
            title = card.locator("h2.position_card_info_title").inner_text()

          company = "Unknown"
          try:
            company_el = card.locator("div[class^='sc-'][class*='-2'] span").first
            company = company_el.inner_text(timeout=1000).strip()
          except Exception as e:
            print(f"âš ï¸ íšŒì‚¬ ì •ë³´ ë¡œë”© ì‹¤íŒ¨: {e}")

          # ìœ„ì¹˜ & ê²½ë ¥ ì •ë³´
          detail_spans = card.locator("ul.cdeuol li")
          details = [detail_spans.nth(j).inner_text(timeout=2000) for j in range(detail_spans.count())]

          link = card.locator("a").first.get_attribute("href")
          if link and not link.startswith("http"):
              link = f"https://jumpit.saramin.co.kr{link}"

          job_data = {
            "title": title.strip(),
            "company": company.strip(),
            "details": details,
            "link": link,
            "source": "jumpit",
            "posted_date": datetime.today().date().isoformat(),
          }

          jobs.append(job_data)

        except Exception as e:
          print(f"âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜: [{i+1}/{count}]:", e)
          continue

    browser.close()
  return jobs

def upload_to_supabase_and_filter_new(jobs):
  headers = {
    "apikey": SUPABASE_KEY,
    "Authorization": f"Bearer {SUPABASE_KEY}",
    "Content-Type": "application/json"
  }

  # ëª¨ë“  link ì¡°íšŒ (limitìœ¼ë¡œ ìˆ˜ ì œí•œ)
  query = f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}?select=link&limit=10000"
  existing = requests.get(query, headers=headers)

  if existing.status_code != 200:
    print("ì¡°íšŒ ì‹¤íŒ¨ supabase", existing.text)
    return []

  existing_links = {item["link"] for item in existing.json()}

  new_jobs = [job for job in jobs if job["link"] not in existing_links]
  update_jobs = [job for job in jobs if job["link"] in existing_links]

  if new_jobs:
    res = requests.post(
      f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}",
      headers=headers,
      json=new_jobs
    )
    if res.status_code not in [200, 201]:
      print("Supabase ì €ì¥ ì‹¤íŒ¨:", res.text)
    else:
      print(f"Supabaseì— ìƒˆ ê³µê³  {len(new_jobs)}ê°œ ì €ì¥ ì™„ë£Œ")

  for job in update_jobs:
    update_url = f"{SUPABASE_URL}/rest/v1/{SUPABASE_TABLE}?link=eq.{job['link']}"
    res = requests.patch(update_url, headers=headers, json=job)
    if res.status_code not in [200, 204]:
      print(f"ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {job['link']} â†’ {res.text}")
    else:
      print(f"ì—…ë°ì´íŠ¸ ì™„ë£Œ: {job['link']}")

  return new_jobs

if __name__ == "__main__":
  job_list = scrape_jumpit_jobs()
  print(f"ì´ {len(job_list)}ê°œ í¬ë¡¤ë§ ì™„ë£Œ")

  for job in job_list:
    print(job["title"], "|", job["company"], "|", job.get("link"))

  new_jobs = upload_to_supabase_and_filter_new(job_list)
  print(f"ğŸ†• ì˜¤ëŠ˜ ìƒˆë¡œ ì¶”ê°€ëœ ê³µê³  {len(new_jobs)}ê°œ:")
  for job in new_jobs:
    print("-", job["title"], "|", job["company"])